\begin{abstract}
    Efficient job resource allocation in seismic data processing is often hindered by the challenge of accurately predicting the best chunk size for a given dataset.
    Although schedulers usually apply a chunking strategy to split the data among workers, the memory usage of such operators may be hard to predict since they can hold intermediate results during execution.
    Determining the optimal chunk size is challenging, as it requires striking a delicate balance between memory usage, computational efficiency, and inter-node communication overhead.
    Gaining insight into an algorithm's memory footprint can significantly simplify this task, enabling better resource allocation, reduced execution time, and improved overall performance.

    This study delves into the impact of different chunk sizes on high-performance seismic processing using DASK.
    Our investigation reveals some noteworthy findings:
    (i) although smaller chunk sizes introduce scheduler overhead, larger chunk sizes escalate memory consumption differently depending on the seismic operator that is being executed;
    (ii) each operator has two different memory consumption patterns, both being while executing with or without memory constraints;
    (iii) although executing an operator with memory pressure is possible, that increases the elapsed time of the execution.

    As a preliminary study, this research lays the groundwork for forthcoming work.
    Among others, those findings may guide future developments of chunk optimization techniques that could explore those and optimize the chunking strategy for seismic processing.
\end{abstract}
