\begin{resumo} 
    Alocar tarefas de forma eficiente em processamento de dados sísmicos é frequentemente dificultado pelo desafio de prever com precisão o melhor tamanho de bloco para um determinado conjunto de dados.
    Embora os escalonadores de tarefas geralmente apliquem uma estratégia de divisão de blocos para lidar com a distribuição dos dados entre os trabalhadores, a quantidade de memória necessária pode ser difícil de prever devido a resultados intermediários durante a execução.
    Determinar o tamanho ideal do bloco é um desafio, pois requer um equilíbrio delicado entre o uso de memória, eficiência computacional e sobrecarga de comunicação entre nós.
    Obter informações sobre o padrão no consumo de memória de um algoritmo pode simplificar significativamente essa tarefa, permitindo uma melhor alocação de recursos, redução do tempo de execução e melhor desempenho geral.

    Este estudo aprofunda o impacto de diferentes tamanhos de bloco no processamento sísmico de alto desempenho usando DASK.
    Nossa investigação revela algumas descobertas notáveis:
    (i) embora bloco menores introduzam sobrecarga no agendador, bloco maiores aumentam o consumo de memória de maneira diferente, dependendo do operador sísmico que está sendo executado;
    (ii) cada operador possui dois padrões de consumo de memória diferentes, sendo quando executados com ou sem restrições de memória;
    (iii) embora a execução de um operador com pressão de memória seja possível, isso aumenta o tempo total de execução.

    Como um estudo preliminar, esta pesquisa lança as bases para trabalhos futuros.
    Entre outros, essas descobertas podem orientar futuros desenvolvimentos de técnicas de otimização de blocos que poderiam explorá-las e otimizar a estratégia de divisão de blocos para o processamento sísmico.
\end{resumo}
